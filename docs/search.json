{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-07-31T23:04:39+02:00"
    },
    {
      "path": "fairness.html",
      "title": "Fairness",
      "author": [],
      "contents": "\n\n\nlibrary(reticulate)\nuse_condaenv('ethique_env')\n\n\n\nFairness\nPour mesurer la partialité d’un algorithme plusieurs métriques ont été proposées. La parité démographique, la parité proportionnelle et l’égalité des chances sont les plus communes pour évaluer la partialité sur les variables sensibles comme le sex ou la nationalité lorsque le modèle est un classifieur binaire. Il existe un très grands nombre d’autre métiques qui dépendent de la matrice de confusion.\nPour illustrer cela nous allons utilisé dans un premier temps les calculers avec les formules explicites puis nous utiliserons la librairie : AI Fairness 360.\nPrépration des données et création du modèle cible\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\nseed = 2021\n\n\ncredit = pd.read_csv(\"data/german_credit_prepared.csv\", sep=\",\", engine=\"python\")\n\ny = credit.default \nX = credit.drop(columns=[\"default\"])\n\n# Définie quelles colones sont categorielles et quelles sont continue\nvariables_cat = [col for col in X.columns if credit[col].dtype==object]\nvariables_ord = [col for col in X.columns if credit[col].dtype==int]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), variables_cat),\n        ('ord', StandardScaler(), variables_ord)\n    ])\n\nmodel = Pipeline(\n        [\n            ('preprocession', preprocess),\n            ('logreg', LogisticRegression())\n        ]\n)\n\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=seed)\n\nlogreg = model.fit(X_train, y_train)\n\nUtilisation de la matrice de confusion\nLa majorité des mesures de partialité sont basées sur la matrice de confusion. Elle comporte quatre classes : - Vrai positif (VP) : la vraie classe est 1 et la classe prédite est 1 - Faux positif (FP) : la vraie classe est 0 et la classe prédite est 1 - Vrai négatif (VN) : la vraie classe est 0 et la classe prédite est 0 - Faux négatif (FN) : la vraie classe est 1 et la classe prédite est 0\nLes métriques de partialité sont calculé à partir de ces 4 valeurs pour des groupes de personnes partagants des caractéristiques communes (sex, ethnicité)\n\ntitle = 'Matrice de confusion de la régression logistique'\ndisp = plot_confusion_matrix(logreg,\n                             X_test,\n                             y_test,\n                             cmap=plt.cm.Blues)\ndisp.ax_.set_title(title)\nText(0.5, 1.0, 'Matrice de confusion de la régression logistique')\nplt.show()\n\n\nMétriques de parité\n\nfrom aif360.datasets import BinaryLabelDataset\nfrom aif360.metrics import ClassificationMetric\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\n\n\ny_test_pred = pd.Series(model.predict(X_test), index=X_test.index, name='defaut')\ny_test = pd.Series(y_test, index=X_test.index, name='defaut')\nsex_test = X_test.sex.map({'male': 0, 'female': 1})\n\nbld_test_pred = BinaryLabelDataset(df=pd.concat([y_test_pred, sex_test], axis=1),\n                                                label_names=['defaut'], \n                                                protected_attribute_names=['sex'],\n                                                favorable_label=0, unfavorable_label=1)\nbld_test = BinaryLabelDataset(df=pd.concat([y_test, sex_test], axis=1),\n                                           label_names=['defaut'], \n                                           protected_attribute_names=['sex'],\n                                           favorable_label=0, unfavorable_label=1)\n\n\nfairness_metrics = ClassificationMetric(bld_test, bld_test_pred, \n                         unprivileged_groups=[{'sex': 1}], privileged_groups=[{'sex': 0}])\n\nAverage of difference in FPR and TPR for unprivileged and privileged groups:\n\\[\\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n   + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\\]\nA value of 0 indicates equality of odds.\n\nfairness_metrics.average_odds_difference()\n-0.14944444444444446\n\n\\[Pr(\\hat{Y} = 1 | D = \\text{unprivileged})\n   - Pr(\\hat{Y} = 1 | D = \\text{privileged})\\]\n\nfairness_metrics.statistical_parity_difference()\n-0.19117647058823528\n\n\\[TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}\\]\n\nfairness_metrics.true_positive_rate_difference()\n-0.16000000000000003\n\n\\[\\frac{Pr(\\hat{Y} = 1 | D = \\text{unprivileged})}\n   {Pr(\\hat{Y} = 1 | D = \\text{privileged})}\\]\n\nfairness_metrics.disparate_impact()\n0.7657657657657658\n\n\nfairness_metrics.accuracy()\n0.755\n\nPost Processing\nhttp://www.datasciencepublicpolicy.org/wp-content/uploads/2021/04/Fairness-Full-Tree.png\n\nfrom aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n\n\ncpp = CalibratedEqOddsPostprocessing(unprivileged_groups=[{'sex': 1}], \n                                   privileged_groups=[{'sex': 0}],\n                                   cost_constraint='fpr')\n\n\ncpp = cpp.fit(bld_test, bld_test_pred)\n\n\nbld_test_pred = cpp.predict(bld_test_pred)\n\n\nfairness_metrics = ClassificationMetric(bld_test, bld_test_pred, \n                         unprivileged_groups=[{'sex': 1}], privileged_groups=[{'sex': 0}])\n\n\nfairness_metrics.average_odds_difference()\n0.10472222222222222\n\n\nfairness_metrics.statistical_parity_difference()\n0.04319852941176472\n\n\nfairness_metrics.true_positive_rate_difference()\n0.015000000000000013\n\n\nfairness_metrics.disparate_impact()\n1.052927927927928\n\n\nfairness_metrics.accuracy()\n0.75\n\nPre Processing\n\nfrom aif360.algorithms.preprocessing import Reweighing\nWARNING:root:No module named 'numba': LFR will be unavailable. To install, run:\npip install 'aif360[LFR]'\n\n\nrw = Reweighing(unprivileged_groups=[{'sex': 1}], \n                privileged_groups=[{'sex': 0}])\n\n\nif 'sex' in variables_cat: variables_cat.remove('sex')\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), variables_cat),\n        ('ord', StandardScaler(), variables_ord)\n    ])\n\nX_train_proc = preprocess.fit_transform(X_train.drop(columns='sex'))\nX_train_proc = pd.concat([pd.DataFrame(X_train_proc, index=X_train.index), \n                          X_train.sex.map({'male':0, 'female':1})], axis=1)\n\n\nbld_train = BinaryLabelDataset(df=pd.concat([X_train_proc, y_train], axis=1),\n                                                label_names=['default'], \n                                                protected_attribute_names=['sex'],\n                                                favorable_label=0, unfavorable_label=1)\n\n\nbld_train = rw.fit_transform(bld_train)\n\n\nw_train = bld_train.instance_weights.ravel()\n\n\nmodel.fit(X_train, y_train, logreg__sample_weight = w_train)\nPipeline(steps=[('preprocession',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first'),\n                                                  ['account_check_status',\n                                                   'credit_history', 'purpose',\n                                                   'savings',\n                                                   'present_emp_since',\n                                                   'personal _status',\n                                                   'other_debtors', 'property',\n                                                   'other_installment_plans',\n                                                   'housing', 'job',\n                                                   'telephone',\n                                                   'foreign_worker']),\n                                                 ('ord', StandardScaler(),\n                                                  ['duration_in_month',\n                                                   'credit_amount',\n                                                   'installment_as_income_perc',\n                                                   'present_res_since', 'age',\n                                                   'credits_this_bank',\n                                                   'people_under_maintenance'])])),\n                ('logreg', LogisticRegression())])\n\n\ny_test_pred = pd.Series(model.predict(X_test), index=X_test.index, name='defaut')\ny_test = pd.Series(y_test, index=X_test.index, name='defaut')\nsex_test = X_test.sex.map({'male': 0, 'female':1})\n\n\n\nbld_test_pred = BinaryLabelDataset(df=pd.concat([y_test_pred, sex_test], axis=1),\n                                                label_names=['defaut'], \n                                                protected_attribute_names=['sex'],\n                                                favorable_label=0, unfavorable_label=1)\nbld_test = BinaryLabelDataset(df=pd.concat([y_test, sex_test], axis=1),\n                                           label_names=['defaut'], \n                                           protected_attribute_names=['sex'],\n                                           favorable_label=0, unfavorable_label=1)\n\n\nfairness_metrics = ClassificationMetric(bld_test, bld_test_pred, \n                         unprivileged_groups=[{'sex': 1}], privileged_groups=[{'sex': 0}])\n\n\nfairness_metrics.average_odds_difference()\n-0.09222222222222223\n\n\nfairness_metrics.statistical_parity_difference()\n-0.13786764705882348\n\n\nfairness_metrics.true_positive_rate_difference()\n-0.11499999999999999\n\n\nfairness_metrics.disparate_impact()\n0.826388888888889\n\n\nfairness_metrics.accuracy()\n0.75\n\n\n\n\n\n\n\n",
      "last_modified": "2021-07-31T23:06:11+02:00"
    },
    {
      "path": "index.html",
      "title": "Ethique",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-07-31T23:06:11+02:00"
    },
    {
      "path": "privacy.html",
      "title": "Privacy",
      "author": [],
      "contents": "\n\n\nlibrary(reticulate)\nuse_condaenv('ethique_env')\n\n\n\nPrivacy et Robusness\nDans ce notebook je vais présenter les différentes attaques présentées dans la trousse à outils d’IMB Adversarial Robusteness 360 :\nEvasion\nPoisoning\nInference and Inversion\nModel Extraction\nPour illustrer ces différentes attaques nous allons utilisé la base de données :\nGerman Credit (lien)\nLe modèle que nous allons attaquer est une régression logisitique en utilisant la librairie scikit learn\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\nfrom itertools import product, combinations\nfrom copy import deepcopy\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport seaborn as sns\nsns.set_theme(style='darkgrid')\n\n\nseed = 2021\n\nPrépration des données et création du modèle cible\n\ncredit = pd.read_csv(\"data/german_credit_prepared.csv\", sep=\",\", engine=\"python\")\n\ny = credit['default']\nX = credit.drop(columns=\"default\")\n\n# Définie quelles colones sont categorielles et quelles sont continue\nvariables_cat = [col for col in X.columns if credit[col].dtype==object]\nvariables_ord = [col for col in X.columns if credit[col].dtype==int]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), variables_cat),\n        ('ord', StandardScaler(), variables_ord)\n    ])\n\nmodel = Pipeline(\n        [\n            ('preprocession', preprocess),\n            ('logreg', LogisticRegression())\n        ]\n)\n\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=seed)\n\nlogreg = model.fit(X_train, y_train)\n\n\ndef mesure_reg(reg, X_test, y_test):\n    y_test_pred = reg.predict(X_test)\n    cm = confusion_matrix(y_test, y_test_pred)\n    total=sum(sum(cm))\n    sensitivity_recall = cm[0,0]/(cm[0,0]+cm[1,0])\n    print('Sensitivity_recall : ',sensitivity_recall )\n    Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n    print('Specificity: ', Specificity)\n    precision = cm[0,0]/(cm[0,0]+cm[0,1])\n    print('Precision: ', precision)\n    accuracy =(cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n    print('Accuracy: ', accuracy)\n    \n    title = 'Matrice de confusion de la régression logistique'\n    disp = plot_confusion_matrix(reg,\n                                 X_test,\n                                 y_test,\n                                 cmap=plt.cm.Blues)\n    disp.ax_.set_title(title)\n    plt.show()\n\n\nmesure_reg(logreg, X_test, y_test)\nSensitivity_recall :  0.8013245033112583\nSpecificity:  0.6122448979591837\nPrecision:  0.8642857142857143\nAccuracy:  0.755\n\n\nEvasion\nOn va chercher les variables non vérifiables et les modifiées jusqu’à ce que l’individu refusé soit accepté.\nSource\nOn voit qu’il n’y a que très peu de variables que l’on peut modifier et qui ne soit pas vérifiable. C’est une base de données pour crédit ce qui explique cela. On voit qu’il y a : - purpose : correspond à la finalité du prêt - personal_status : la condition de vie en couple\nOn a également des variables difficilements vérifiables : - present_emp_since : depuis quand la personne travail à son poste - job : le type de travaille de la personne - people_under_maintenance : le nombre de personne à charge\nOn choisit ensuite une personne aléatoirement parmis les personnes refusées :\n\n#np.random.seed(seed)\nidx_ref = np.where(logreg.predict_proba(X)[:, 1] > 0.5)[0]\nidx = np.random.choice(idx_ref)\n\n#idx = 153 # --> juste besoin de changer le but du prêt : 'education', '(vacation - does not exist?)', 'car (new)'\n#idx = 232 #--> pas possible de le faire passer\nX.iloc[idx]\naccount_check_status                                          0 <= ... < 200 DM\nduration_in_month                                                            24\ncredit_history                                  delay in paying off in the past\npurpose                                                               car (new)\ncredit_amount                                                              1965\nsavings                                             unknown/ no savings account\npresent_emp_since                                            1 <= ... < 4 years\ninstallment_as_income_perc                                                    4\nsex                                                                      female\npersonal _status                                                       divorced\nother_debtors                                                              none\npresent_res_since                                                             4\nproperty                      if not A121/A122 : car or other, not in attrib...\nage                                                                          42\nother_installment_plans                                                    none\nhousing                                                                    rent\ncredits_this_bank                                                             2\njob                                                 skilled employee / official\npeople_under_maintenance                                                      1\ntelephone                              yes, registered under the customers name\nforeign_worker                                                              yes\nName: 591, dtype: object\n\n\nlogreg.predict_proba(X.iloc[idx].to_frame().transpose())[:, 1]\narray([0.60792326])\n\n\nvar_values = ['purpose', 'personal _status', 'present_emp_since', 'job', 'people_under_maintenance']\nfix_values = [x for x in X.columns if x not in var_values]\n\nall_values = [X[col].unique() for col in var_values]\nprod = list(product(*all_values))   #produit cartésien\n\nall_X = pd.DataFrame([X.iloc[idx][fix_values].tolist() + list(p) for p in prod],\n                      columns=fix_values + var_values)\nall_X = all_X[X.columns]\nall_X['proba'] = logreg.predict_proba(all_X)[:, 1]\n\n\nall_X.sort_values('proba', inplace=True)\nvalid_X = all_X[all_X.proba < 0.5]\n\n# On calcule le nombre de variable qu'on a du modifier\nvalid_X = valid_X.assign(diff = [(valid_X.iloc[i][var_values] == X.iloc[idx][var_values]).value_counts()[False] \n                                       for i in range(len(valid_X))])\n\nvalid_X.sort_values(by=['diff', 'proba'], inplace=True)\nvalid_X.head(5)\n     account_check_status  duration_in_month  ...     proba diff\n528     0 <= ... < 200 DM                 24  ...  0.210769    1\n48      0 <= ... < 200 DM                 24  ...  0.389484    1\n1008    0 <= ... < 200 DM                 24  ...  0.416004    1\n1128    0 <= ... < 200 DM                 24  ...  0.419004    1\n368     0 <= ... < 200 DM                 24  ...  0.440051    1\n\n[5 rows x 23 columns]\n\n\ndef print_diff(new_comb, old_comb):\n    diff_comb = (new_comb != old_comb)\n    diff_pd = pd.DataFrame([old_comb[diff_comb], new_comb[diff_comb]], index=['old', 'new']).transpose()\n\n    max_ind = max([len(x) for x in diff_pd.index])\n    max_old = max([len(x) for x in diff_pd.old])\n    max_new = max([len(x) for x in diff_pd.new])\n\n    for i in new_comb[diff_comb].index :\n        print(f'{i:<{max_ind}} : {diff_pd.old[i]:<{max_old}} --> {diff_pd.new[i]:<{max_new}}')\n\n\nprint_diff(valid_X.iloc[0][var_values], X.iloc[idx][var_values])\npurpose : car (new) --> car (used)\n\n\nX_ref = X.loc[idx_ref]\n\n\nnew = False\n\nif new:\n    all_X = pd.DataFrame([X.iloc[idx][fix_values].tolist() + list(p)  + [idx] for idx in tqdm(idx_ref) for p in prod],\n                         columns=fix_values + var_values + ['index_old'])\n    all_X = all_X[list(X.columns) + ['index_old']]\n    all_X['proba'] = logreg.predict_proba(all_X.drop('index_old', axis=1))[:, 1]\n    np.savez_compressed('data/privacy/all_X.npz', data=np.array(all_X), columns=all_X.columns)\nelse:\n    file = np.load('data/privacy/all_X.npz', allow_pickle=True)\n    all_X = pd.DataFrame(file['data'], columns=file['columns'])\n\nX_false = X.loc[idx_ref]\nX_false = X_false.assign(proba = logreg.predict_proba(X_false)[:, 1])\n\nX_false = X_false.assign(proba_min = all_X.groupby('index_old').apply(lambda x: x.sort_values('proba').head(1)) \\\n                                                                .set_index('index_old').proba)\n\n\nlen(X_false[X_false.proba_min < 0.5])\n212\n\n\n(X_false.proba - X_false.proba_min).plot.hist()\n<AxesSubplot:ylabel='Frequency'>\nplt.show()\n\n\n\nX_false[X_false.proba_min >= 0.5]\n    account_check_status  duration_in_month  ...     proba proba_min\n29                < 0 DM                 60  ...  0.864682   0.52768\n95     0 <= ... < 200 DM                 54  ...  0.930303  0.664193\n334               < 0 DM                 24  ...  0.866776  0.519487\n374    0 <= ... < 200 DM                 60  ...  0.950675  0.706918\n538               < 0 DM                 48  ...  0.940321  0.641234\n650               < 0 DM                 48  ...  0.819160  0.506383\n714    0 <= ... < 200 DM                 60  ...  0.930525    0.6976\n728    0 <= ... < 200 DM                 48  ...  0.925651  0.524549\n832               < 0 DM                 45  ...  0.874828  0.549941\n927               < 0 DM                 48  ...  0.772503  0.723198\n973               < 0 DM                 60  ...  0.937913  0.725362\n\n[11 rows x 23 columns]\n\nPoisoning\nOn remarque qu’il y a certaine personne pour qui la modification de variables difficilement vérifiables n’est pas suffisant. On passe donc à la deuxième attaque poisoning. On a vu précédemment que l’individu 232 fait partie des personnes ne pouvant pas utiliser la triche pour pouvoir être accepté.\nOn va rajouter sa ligne dans la base de donnée d’entrainement et voir si cela a un impact.\n\ny = credit['default']\nX = credit.drop(columns=\"default\")\n\nind = X.iloc[[95]]\n\n\nind\n   account_check_status  ...  foreign_worker\n95    0 <= ... < 200 DM  ...             yes\n\n[1 rows x 21 columns]\n\n\nn = 9\nX_train_pois = X_train.append([ind]*n, ignore_index=True)\ny_train_pois = pd.Series(list(y_train) + [0]*n)\n\n\nlogreg = model.fit(X_train, y_train)\n\nmesure_reg(logreg, X_test, y_test)\nSensitivity_recall :  0.8013245033112583\nSpecificity:  0.6122448979591837\nPrecision:  0.8642857142857143\nAccuracy:  0.755\n\n\n\nlogreg.predict_proba(ind)[0,1]\n0.9303033801912368\n\n\nlogreg_pois = model.fit(X_train_pois, y_train_pois)\n\nmesure_reg(logreg_pois, X_test, y_test)\nSensitivity_recall :  0.7806451612903226\nSpecificity:  0.5777777777777777\nPrecision:  0.8642857142857143\nAccuracy:  0.735\n\n\n\nlogreg_pois.predict_proba(ind)[0,1]\n0.47122487407898306\n\nOn automatise la recherche du nombre minimum de copie pour passer.\n\ndef min_pois(ind):\n    n = 0\n    X_train_pois = X_train\n    y_train_pois = y_train\n    \n    while model.fit(X_train_pois, y_train_pois).predict_proba(ind)[0,1] > 0.5:\n        n += 1\n        X_train_pois = X_train_pois.append([ind], ignore_index=True)\n        y_train_pois = pd.Series(list(y_train_pois) + [0])\n        \n    return n\n\n\nmin_pois(ind)\n9\n\nOn regarde la distribution pour les personnes qui ont besoin du poisoning\n\nlist_min = X_false[X_false.proba_min > 0.5].drop(columns=['proba', 'proba_min']).apply(lambda x: min_pois(x.to_frame().transpose()), axis=1)\n\n\nlist_min.plot.hist()\n<AxesSubplot:ylabel='Frequency'>\nplt.show()\n\n\nOn peut coupler les deux premières attaques pour avoir encore moins de copie à rajouter.\n\nind_best_comb = all_X[all_X['index_old'] == ind.index[0]].sort_values(by='proba').head(1)  \\\n                                            .drop(['index_old', 'proba'], axis=1)\n\n\nprint_diff(ind_best_comb.iloc[0], ind.iloc[0])\npurpose           : business                    --> car (used)                                                   \npresent_emp_since : ... < 1 year                --> 4 <= ... < 7 years                                           \njob               : skilled employee / official --> management/ self-employed/ highly qualified employee/ officer\n\n\nn = 2\nX_train_pois_best = X_train.append([ind_best_comb]*n, ignore_index=True)\ny_train_pois_best = pd.Series(list(y_train) + [0]*n)\n\n\nlogreg_pois_best = model.fit(X_train_pois_best, y_train_pois_best)\n\nmesure_reg(logreg_pois_best, X_test, y_test)\nSensitivity_recall :  0.7894736842105263\nSpecificity:  0.5833333333333334\nPrecision:  0.8571428571428571\nAccuracy:  0.74\n\n\n\nlogreg_pois_best.predict_proba(ind_best_comb)[0,1]\n0.4932345933281425\n\nOn automatise\n\ndef min_pois_best_comb(ind, print_chan=True):\n    ind_best_comb = all_X[all_X['index_old'] == ind.index[0]].sort_values(by='proba').head(1)  \\\n                                            .drop(['index_old', 'proba'], axis=1)\n\n    print_diff(ind_best_comb.iloc[0], ind.iloc[0]) if print_chan else ...\n    \n    n = 0\n    X_train_pois = X_train\n    y_train_pois = y_train\n    \n    while model.fit(X_train_pois, y_train_pois).predict_proba(ind_best_comb)[0,1] > 0.5:\n        n += 1\n        X_train_pois = X_train_pois.append([ind_best_comb], ignore_index=True)\n        y_train_pois = pd.Series(list(y_train_pois) + [0])\n        \n    return n\n\n\nmin_pois_best_comb(ind)\npurpose           : business                    --> car (used)                                                   \npresent_emp_since : ... < 1 year                --> 4 <= ... < 7 years                                           \njob               : skilled employee / official --> management/ self-employed/ highly qualified employee/ officer\n2\n\n\n# %%capture\nlist_min_best_comb = X_false[X_false.proba_min > 0.5].apply(lambda x: \n                                                            min_pois_best_comb(x.to_frame().transpose(), False), axis=1)\n\n\nlist_min_best_comb.plot.hist()\n<AxesSubplot:ylabel='Frequency'>\nplt.show()\n\n\nModel extraction\nLe but de cette attaque est de réccupérer les poids d’un modèle. On suppose que l’on connait l’architechure : regression logistique.\nCe que l’on va faire c’est créer des nouvelles entrées que l’on va donné à notre modèle blackbox qui va les labélisé puis on va utilser ces labels pour entrainer notre nouveau modèle.\n\ndef new_entries(n):\n    list_entries = []\n    for col in X.columns:\n        list_entries.append(np.random.choice(X[col].unique(), n, p=(X[col].value_counts(normalize=True)).to_numpy()))\n    return pd.DataFrame(np.array(list_entries).transpose(), columns=X.columns)\n\n\nn = len(X_train)\nX_new = new_entries(n)\ny_new = logreg.predict(X_new)\n\n\nmesure_reg(model.fit(X_train, y_train), X_test, y_test)\nSensitivity_recall :  0.8013245033112583\nSpecificity:  0.6122448979591837\nPrecision:  0.8642857142857143\nAccuracy:  0.755\n\n\n\nmesure_reg(model.fit(X_new, y_new), X_test, y_test)\nSensitivity_recall :  0.7922077922077922\nSpecificity:  0.6086956521739131\nPrecision:  0.8714285714285714\nAccuracy:  0.75\n\n\n\ndef new_entries_unif(n):\n    list_entries = []\n    for col in X.columns:\n        list_entries.append(np.random.choice(X[col].unique(), n))\n    return pd.DataFrame(np.array(list_entries).transpose(), columns=X.columns)\n\n\nn = len(X_train)\nX_new_unif = new_entries_unif(n)\nlogreg = model.fit(X_train, y_train)\ny_new_unif = logreg.predict(X_new_unif)\n\n\nmesure_reg(model.fit(X_new_unif, y_new_unif), X_test, y_test)\nSensitivity_recall :  0.8053691275167785\nSpecificity:  0.6078431372549019\nPrecision:  0.8571428571428571\nAccuracy:  0.755\n\n\nMesure de distance : - l’accuracy : mesure de l’efficacité de l’apprentissage - la précision : mesure de la distance du modèle original\n\ny_hat = model.fit(X_new, y_new).predict(X_test)\ny_ori = model.fit(X_train, y_train).predict(X_test)\n\nprec = (y_ori == y_hat).astype(int).sum()/len(y_ori)\naccu_diff = accuracy_score(y_test, y_ori) - accuracy_score(y_test, y_hat)\nprint(f\"Précision : {prec:.3f}, Différence d'accuracy : {accu_diff:.3f}\")\nPrécision : 0.905, Différence d'accuracy : 0.005\n\n\ny_hat = model.fit(X_new_unif, y_new_unif).predict(X_test)\ny_ori = model.fit(X_train, y_train).predict(X_test)\n\nprec = (y_ori == y_hat).astype(int).sum()/len(y_ori)\naccu_diff = accuracy_score(y_test, y_ori) - accuracy_score(y_test, y_hat)\nprint(f\"Précision : {prec:.3f}, Différence d'accuracy : {accu_diff:.3f}\")\nPrécision : 0.960, Différence d'accuracy : 0.000\n\nTrouver un modèle capable de surapprendre\n\ndef add_relations(df_orig):\n    df = df_orig.copy()\n    for i, j in combinations(variables_cat, 2):\n        df[i + j] = df[i] + df[j]\n    return df\n\nnew_variables_cat = variables_cat + [i + j for i, j in combinations(variables_cat, 2)]\n\n\n# On normalise les colones dans leur noms\nall_values = [X[col].unique() for col in variables_cat]\ncategorie_rel = list(combinations(all_values, 2))\n\ncategorie_name = [a + b for a,b in combinations(variables_cat, 2)]\ncategorie_values = [[a + b for a,b in list(product(*x))] for x in categorie_rel]\n\npreprocessor_comb = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(categories=all_values+categorie_values, drop='first', sparse=False), \n         new_variables_cat),\n        ('ord', StandardScaler(), variables_ord)\n    ])\n\nmodel_over = Pipeline(\n    steps=[\n        ('relations', FunctionTransformer(add_relations)), \n        ('preprocession', preprocessor_comb),\n        ('logreg', LogisticRegression(max_iter=200))\n    ]\n)\n\n\n# #%timeit model.fit(X_train, y_train)\n#print(f\"validation : {model.score(X_test, y_test):.3f}, score : {model.score(X_train, y_train):.3f}\") \n\nprint(\"\"\"39.1 ms ± 2.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nvalidation : 0.735, score : 0.787\"\"\")\n39.1 ms ± 2.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nvalidation : 0.735, score : 0.787\n\n\n# #%timeit model_over.fit(X_train, y_train)\n#print(f\"validation : {model_over.score(X_test, y_test):.3f}, score : {model_over.score(X_train, y_train):.3f}\") \n\nprint(\"\"\"216 ms ± 21.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nvalidation : 0.725, score : 0.958\"\"\")\n216 ms ± 21.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nvalidation : 0.725, score : 0.958\n\nOn a donc de l’overfit\nProprety inference\nOn va chercher a reccupérer la proportion homme/femme de la base de donnée d’entrainement\n\ndist = np.random.normal(np.random.choice([0.4, 0.6], size=1000), scale=0.05)\nplt.hist(dist, density=True, bins=50)\n(array([0.10510399, 0.10510399, 0.42041596, 0.84083192, 0.42041596,\n       0.42041596, 0.52551995, 0.9459359 , 1.15614388, 2.83780771,\n       1.89187181, 2.52249575, 3.25822367, 3.15311968, 4.51947154,\n       3.04801569, 4.83478351, 3.36332766, 4.09905559, 2.83780771,\n       1.89187181, 3.15311968, 1.26124787, 1.9969758 , 1.47145585,\n       1.05103989, 0.84083192, 1.05103989, 1.26124787, 1.78676782,\n       2.73270372, 2.83780771, 3.04801569, 3.04801569, 3.36332766,\n       3.88884761, 4.41436756, 4.30926357, 3.15311968, 2.9429117 ,\n       2.9429117 , 2.73270372, 1.9969758 , 1.89187181, 1.36635186,\n       1.36635186, 0.52551995, 0.63062394, 0.52551995, 0.31531197]), array([0.25426357, 0.26377796, 0.27329234, 0.28280673, 0.29232112,\n       0.3018355 , 0.31134989, 0.32086428, 0.33037866, 0.33989305,\n       0.34940744, 0.35892182, 0.36843621, 0.3779506 , 0.38746498,\n       0.39697937, 0.40649376, 0.41600814, 0.42552253, 0.43503692,\n       0.4445513 , 0.45406569, 0.46358008, 0.47309446, 0.48260885,\n       0.49212324, 0.50163762, 0.51115201, 0.5206664 , 0.53018079,\n       0.53969517, 0.54920956, 0.55872395, 0.56823833, 0.57775272,\n       0.58726711, 0.59678149, 0.60629588, 0.61581027, 0.62532465,\n       0.63483904, 0.64435343, 0.65386781, 0.6633822 , 0.67289659,\n       0.68241097, 0.69192536, 0.70143975, 0.71095413, 0.72046852,\n       0.72998291]), <BarContainer object of 50 artists>)\nplt.show()\n\n\n\ndef new_entries_homme(n, prop=False):\n    list_entries = []\n    for col in X.columns:\n        if col == 'sex':\n            q = np.clip(np.random.normal(np.random.choice([0.4, 0.6]), scale=0.05), 0, 1)\n            list_entries.append(np.random.choice(['male', 'female'], n, p=[q, 1-q]))\n        else:\n            if prop:\n                list_entries.append(np.random.choice(X[col].unique(), n, p=(X[col].value_counts(normalize=True)).to_numpy()))\n            else:\n                list_entries.append(np.random.choice(X[col].unique(), n))\n    return pd.DataFrame(np.array(list_entries).transpose(), columns=X.columns)\n\n\nnew_entries_homme(1000).sex.value_counts(normalize=True)\nfemale    0.546\nmale      0.454\nName: sex, dtype: float64\n\nOn se place dans le cas suivant :\nOn a un modèle model_secret qui à la structure de model_chose et qui est entrainé sur une base de données : X_train_secret, et une labélisation : y_train_secret. On cherche à savoir si la base de données secrète avait une majorité d’homme.\nPour se faire on va procéder de la manière suivante :\nComme on n’a pas accès à la base de données X_train_secret on va devoir créer une liste de bases : list_data_X qui est une liste de base générée avec la fonction : new_entries_homme qui génére de nouvelles entrées uniformément pour toutes les variables sauf sex où la proportion d’hommes est tirée selon une normale centré en 0,6 ou 0,4 et de écart type 0,05. On utilise le modèle model_secret pour labéliser les nouvelles entrées que l’on garde dans la variable liste_data_y. (étape de model extraction)\nPour chaque nouvelle base de données on entraine un modèle de la structure model_choisi et on récupère les poids de ce modèle. Tous ces poids forme la base data_meta_X. C’est une base de données où chaque entrée \\(i\\) coorespond aux coefficients du modèle entrainé sur (list_data_X[i], list_data_y[i]).\nOn crée la labélisation de notre méta-modèle data_meta_y en notant \\(1\\) s’il y a plus d’homme que de femme dans le base de données list_data_X[i] et 0 sinon.\nOn va entrainer notre méta-modèle qui est une régression logstique sur (data_meta_X, data_meta_y)\nOn va finalement donner les coefficients de notre modèle secret à notre méta-modèle et il va nous dire si le modèle secret a été entrainé sur une base de données avec une majoritée d’homme (1) ou de femme (0).\nFAIRE UN SCHÉMA\n\nn_data = 300\nn_entries = len(X_train)\n\nX_train_secret = X_train\ny_train_secret = y_train\n\nprop = False\nnew = False\n# Choisir si on veut utiliser le modèle qui surapprend ou pas \nmodel_choisi = model  #model\nname = '_overfit' if model_choisi == model_over else ''\nprop_name = '_prop' if prop else '_unif'\n\nmodel_secret = deepcopy(model_choisi.fit(X_train_secret, y_train_secret))\n\nif new:\n    list_data_X = [new_entries_homme(n_entries, prop) for _ in tqdm(range(n_data))]\n    dist_homme = np.array([(x.sex.value_counts(normalize=True))['male'] for x in list_data_X])\n    \n    list_data_y = model_secret.predict(pd.DataFrame(np.array(list_data_X).reshape(-1, 21), \n                                               columns=X.columns)).reshape(n_data, -1)\n\n    data_meta_X = [model_choisi.fit(list_data_X[i], list_data_y[i])['logreg'].coef_[0] \n                       for i in tqdm(range(len(list_data_X)))]\n    \n    np.savez_compressed(f'data/privacy/data{name}{prop_name}.npz', \n                        data_meta=data_meta_X, \n                        list_homme=dist_homme)\nelse:\n    file = np.load(f'data/privacy/data{name}{prop_name}.npz', allow_pickle=True)\n    data_meta_X = file['data_meta']\n    dist_homme = file['dist_homme']\n    \ndata_meta_y = (dist_homme > 0.5).astype('int')\n\nVisualisation de la répartition de la proportion d’hommes dans les bases de données\n\nplt.hist(dist_homme, density=True, bins=50)\n(array([0.74487896, 0.37243948, 0.        , 0.74487896, 0.37243948,\n       3.35195531, 2.23463687, 2.23463687, 3.35195531, 2.97951583,\n       2.23463687, 4.46927374, 3.35195531, 2.23463687, 4.46927374,\n       2.60707635, 2.97951583, 2.23463687, 7.44878957, 0.74487896,\n       1.48975791, 1.11731844, 1.86219739, 1.48975791, 0.        ,\n       1.86219739, 1.86219739, 2.23463687, 1.48975791, 0.74487896,\n       1.11731844, 2.97951583, 2.23463687, 2.23463687, 3.72439479,\n       2.60707635, 2.60707635, 5.95903166, 5.95903166, 2.97951583,\n       5.95903166, 1.48975791, 0.74487896, 1.86219739, 1.48975791,\n       1.11731844, 1.11731844, 0.74487896, 0.        , 1.48975791]), array([0.2725 , 0.28145, 0.2904 , 0.29935, 0.3083 , 0.31725, 0.3262 ,\n       0.33515, 0.3441 , 0.35305, 0.362  , 0.37095, 0.3799 , 0.38885,\n       0.3978 , 0.40675, 0.4157 , 0.42465, 0.4336 , 0.44255, 0.4515 ,\n       0.46045, 0.4694 , 0.47835, 0.4873 , 0.49625, 0.5052 , 0.51415,\n       0.5231 , 0.53205, 0.541  , 0.54995, 0.5589 , 0.56785, 0.5768 ,\n       0.58575, 0.5947 , 0.60365, 0.6126 , 0.62155, 0.6305 , 0.63945,\n       0.6484 , 0.65735, 0.6663 , 0.67525, 0.6842 , 0.69315, 0.7021 ,\n       0.71105, 0.72   ]), <BarContainer object of 50 artists>)\nplt.show()\n\n\nEntrainement du méta-modèle\n\nX_train_meta, X_test_meta, y_train_meta, y_test_meta = model_selection.train_test_split(data_meta_X, \n                                                                                        data_meta_y, \n                                                                                        test_size=0.20)\nmeta_model = LogisticRegression(max_iter=200)\nmeta_model.fit(X_train_meta, y_train_meta)\nLogisticRegression(max_iter=200)\n\n\nconfusion_matrix(y_test_meta, meta_model.predict(X_test_meta))\narray([[16, 15],\n       [14, 15]])\n\n\nl = []\nX_train_meta_all, X_test_meta_all, y_train_meta_all, y_test_meta_all = model_selection.train_test_split(data_meta_X, \n                                                                                        data_meta_y, \n                                                                                        test_size=0.01)\nfor i in tqdm(range(100)):\n    X_train_meta, _, y_train_meta, _ = model_selection.train_test_split(X_train_meta_all, \n                                                                        y_train_meta_all, \n                                                                        test_size=0.20)\n    meta_model = LogisticRegression(max_iter=200)\n    meta_model.fit(X_train_meta, y_train_meta)\n    \n    #l += [meta_model.predict_proba(model_secret['logreg'].coef_)[0,1]]\n    l += [meta_model.predict_proba([X_test_meta[0]])[0,1]]\n  0%|          | 0/100 [00:00<?, ?it/s]\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nLogisticRegression(max_iter=200)\nplt.hist(l, density=True, bins=50, color='b')\n(array([ 1.32828659,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  1.32828659,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  2.65657318,  0.        ,  3.98485976,  0.        ,\n        1.32828659,  5.31314635,  1.32828659,  0.        ,  1.32828659,\n        2.65657318,  3.98485976,  3.98485976,  2.65657318,  2.65657318,\n        2.65657318,  5.31314635,  6.64143294,  2.65657318,  5.31314635,\n        2.65657318,  3.98485976,  6.64143294, 10.6262927 ,  7.96971953,\n        3.98485976,  3.98485976, 10.6262927 ,  6.64143294,  1.32828659,\n        1.32828659,  7.96971953,  2.65657318,  2.65657318,  2.65657318]), array([0.35235944, 0.35988794, 0.36741643, 0.37494493, 0.38247342,\n       0.39000192, 0.39753041, 0.40505891, 0.41258741, 0.4201159 ,\n       0.4276444 , 0.43517289, 0.44270139, 0.45022989, 0.45775838,\n       0.46528688, 0.47281537, 0.48034387, 0.48787236, 0.49540086,\n       0.50292936, 0.51045785, 0.51798635, 0.52551484, 0.53304334,\n       0.54057183, 0.54810033, 0.55562883, 0.56315732, 0.57068582,\n       0.57821431, 0.58574281, 0.59327131, 0.6007998 , 0.6083283 ,\n       0.61585679, 0.62338529, 0.63091378, 0.63844228, 0.64597078,\n       0.65349927, 0.66102777, 0.66855626, 0.67608476, 0.68361326,\n       0.69114175, 0.69867025, 0.70619874, 0.71372724, 0.72125573,\n       0.72878423]), <BarContainer object of 50 artists>)\nplt.show()\n\n\n\nprint(y_test_meta[0])\n1\n\nRégression linéaire pour trouver le proportion d’homme\n\ndata_meta_y_linear = dist_homme\nX_train_meta, X_test_meta, y_train_meta, y_test_meta = model_selection.train_test_split(data_meta_X, \n                                                                                        data_meta_y_linear, \n                                                                                        test_size=0.20, \n                                                                                        random_state=seed)\n\n\nmeta_model_linear = LinearRegression()\nmeta_model_linear.fit(X_train_meta, y_train_meta)\nLinearRegression()\n\nOn regarde la différence moyenne en pourcentage entre la prévision du modèle et la réalité\n\n((np.abs(y_test_meta - meta_model_linear.predict(X_test_meta))/y_test_meta) * 100).mean()\n24.322101347546102\n\n\nmeta_model_linear.predict(model_secret['logreg'].coef_)\narray([-0.00056166])\n\n\nfrom sklearn import svm\n\nmeta_model_svr = svm.LinearSVR(max_iter=100000)\nmeta_model_svr.fit(X_train_meta, y_train_meta)\nLinearSVR(max_iter=100000)\n\n\n((np.abs(y_test_meta - meta_model_svr.predict(X_test_meta))/y_test_meta) * 100).mean()\n26.66214480664375\n\n\nmeta_model_svr.predict(model_secret['logreg'].coef_)\narray([0.15982339])\n\nDifferential Privacy\nOn utilise la librairie diffprivlib qui permet d’implémenter avec des modèles de sklearn la differential privacy.\n\nimport diffprivlib.models as diff\n\n\nepsilon=1\ndata_norm=1\n\nmodel_over_diff = Pipeline(\n    steps=[\n        ('relations', FunctionTransformer(add_relations)), \n        ('preprocession', preprocessor_comb),\n        # La seule différence avec model_over\n        ('logreg', diff.LogisticRegression(max_iter=200, epsilon=epsilon, data_norm=data_norm))\n    ]\n)\n\n\nmodel_over_diff.fit(X_train, y_train)\nPipeline(steps=[('relations',\n                 FunctionTransformer(func=<function add_relations at 0x7fed242241f0>)),\n                ('preprocession',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[array(['< 0 DM', '0 <= ... < 200 DM', 'no checking account',\n       '>= 200 DM / salary assignments for at least 1 year'], dtype=object),\n                                                                            array(['critical account/ other credits existing (not at this bank)',\n       'exi...\n                                                   'credit_historypurpose',\n                                                   'credit_historysavings',\n                                                   'credit_historypresent_emp_since', ...]),\n                                                 ('ord', StandardScaler(),\n                                                  ['duration_in_month',\n                                                   'credit_amount',\n                                                   'installment_as_income_perc',\n                                                   'present_res_since', 'age',\n                                                   'credits_this_bank',\n                                                   'people_under_maintenance'])])),\n                ('logreg',\n                 LogisticRegression(accountant=BudgetAccountant(spent_budget=[(1, 0)]),\n                                    data_norm=1, epsilon=1, max_iter=200))])\nprint(f\"validation : {model_over_diff.score(X_test, y_test):.3f}, score : {model_over_diff.score(X_train, y_train):.3f}\") \nvalidation : 0.340, score : 0.345\n\nC’est là où il y a un problème on ne peut plus vraiment utilisé le modèle secret pour auto-labélisé les nouvelles données car il perd en efficacité donc la labélisation de y_train_secret est vraiment différentes de l’auto-labélisation. Il faut donc s’éloigner de la situation initale qui combinait le proprety inference et le model extraction comme on ne veut que le premier on va suivre le protocole suivant :\nOn va créer un pool de données (100000)\nOn utilise le model_secret entrainé sur (X_train_secret, y_train_secret) pour auto-labélisé\nOn va tirer une base de données secrete parmis ces données que l’on va renommé data_secret\nLa liste de data va être tirée dans la pool en tirant le bon nombre d’entrées hommme et femme\nOn entraine de la même manière le meta modèle mais cette fois-ci : y_train_secret et le y de list_data_y sont de même forme\nCreation d’un très grand nombre d’entrées\n\ndef new_entries_unif_sex(n):\n    list_entries = []\n    for col in X.columns:\n        if col == 'sex':\n            list_entries.append(np.random.choice(X[col].unique(), n))\n        else:\n            list_entries.append(np.random.choice(X[col].unique(), n, p=(X[col].value_counts(normalize=True)).to_numpy()))\n    return pd.DataFrame(np.array(list_entries).transpose(), columns=X.columns)\n\n\ndef index_fix_p_homme(p, N):\n    nb_homme = round(p * N) \n\n    male_index = np.random.choice(all_male_index, replace=False, size=nb_homme)\n    female_index = np.random.choice(all_female_index, replace=False, size=N-nb_homme)\n\n    return np.concatenate([male_index, female_index])\n\n\nN = 800\nm = 300\n\nnew = False\n\nmodel_predict = model_over.fit(X_train, y_train)\n\nif new:  \n    data_pool = new_entries_unif_sex(100000)\n    data_pool['defaut'] = model_predict.predict(data_pool)\n    \n    all_male_index = data_pool[data_pool.sex == 'male'].index\n    all_female_index = data_pool[data_pool.sex == 'female'].index\n        \n    all_p = np.clip(np.random.normal(np.random.choice([0.4, 0.6], size=m), scale=0.05), 0, 1)\n    sub_index = [index_fix_p_homme(p, N) for p in tqdm(all_p)]\n    np.savez_compressed('data/privacy/diff_data.npz', data_pool=np.array(data_pool), \n                        columns_pool=data_pool.columns, sub_index=np.array(sub_index))\nelse:\n    file = np.load('data/privacy/diff_data.npz', allow_pickle=True)\n    \n    data_pool = pd.DataFrame(file['data_pool'], columns=file['columns_pool'])\n    sub_index = file['sub_index']\n    \nsub_data = [data_pool.iloc[indexes] for indexes in sub_index]\n\n\ndist_homme = [(x.sex.value_counts(normalize=True))['male'] for x in sub_data]\nplt.hist(dist_homme, density=True, bins=50)\n(array([0.38314176, 0.38314176, 0.38314176, 0.76628352, 1.14942529,\n       2.29885057, 1.14942529, 3.44827586, 1.91570881, 1.91570881,\n       4.59770115, 2.29885057, 4.98084291, 5.36398467, 6.89655172,\n       1.53256705, 2.68199234, 3.83141762, 0.76628352, 2.29885057,\n       1.91570881, 2.68199234, 1.14942529, 3.0651341 , 1.14942529,\n       0.76628352, 0.        , 1.53256705, 1.14942529, 1.53256705,\n       0.76628352, 2.68199234, 3.44827586, 1.14942529, 3.0651341 ,\n       4.59770115, 5.74712644, 4.98084291, 3.83141762, 4.59770115,\n       3.83141762, 3.44827586, 2.68199234, 1.14942529, 1.91570881,\n       1.53256705, 0.38314176, 0.38314176, 0.        , 0.76628352]), array([0.27625, 0.28495, 0.29365, 0.30235, 0.31105, 0.31975, 0.32845,\n       0.33715, 0.34585, 0.35455, 0.36325, 0.37195, 0.38065, 0.38935,\n       0.39805, 0.40675, 0.41545, 0.42415, 0.43285, 0.44155, 0.45025,\n       0.45895, 0.46765, 0.47635, 0.48505, 0.49375, 0.50245, 0.51115,\n       0.51985, 0.52855, 0.53725, 0.54595, 0.55465, 0.56335, 0.57205,\n       0.58075, 0.58945, 0.59815, 0.60685, 0.61555, 0.62425, 0.63295,\n       0.64165, 0.65035, 0.65905, 0.66775, 0.67645, 0.68515, 0.69385,\n       0.70255, 0.71125]), <BarContainer object of 50 artists>)\nplt.show()\n\n\n\nnew = False\n\nlist_data_X = [df.drop('defaut', axis=1) for df in sub_data]\n\nif new:\n    list_data_y = [df.defaut for df in sub_data]\n\n    data_meta_X = [model_over.fit(list_data_X[i], list_data_y[i])['logreg'].coef_[0] \n                        for i in tqdm(range(len(list_data_X)))]\n\n    np.savez_compressed('data/privacy/data_meta_over_prop_diff.npz', data_meta=np.array(data_meta_X))\nelse:\n    data_meta_X = np.load('data/privacy/data_meta_over_prop_diff.npz')['data_meta']\n    \n    \ndata_meta_y = np.array([df.sex.value_counts().index[0] == 'male' \n                            for df in list_data_X]).astype('int')\n\n\nX_open, X_secret, y_open, y_secret = model_selection.train_test_split(data_meta_X, \n                                                                        data_meta_y, \n                                                                        test_size=0.20)\n\nmeta_model = LogisticRegression(max_iter=200)\nmeta_model.fit(X_open, y_open)\nLogisticRegression(max_iter=200)\nconfusion_matrix(y_secret, meta_model.predict(X_secret))\narray([[16, 16],\n       [15, 13]])\n\n\nall_male_index = data_pool[data_pool.sex == 'male'].index\nall_female_index = data_pool[data_pool.sex == 'female'].index\n\nX_secret = data_pool.iloc[index_fix_p_homme(0.6, 1000)].drop(columns=['defaut'])\ny_secret = model_predict.predict(X_secret)\n      \nX_secret_train, X_secret_test, y_secret_train, y_secret_test = model_selection.train_test_split(X_secret, \n                                                                                                    y_secret, \n                                                                                                    test_size=0.20)\n\n\nmodel_over.fit(X_secret_train, y_secret_train)\nPipeline(steps=[('relations',\n                 FunctionTransformer(func=<function add_relations at 0x7fed242241f0>)),\n                ('preprocession',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[array(['< 0 DM', '0 <= ... < 200 DM', 'no checking account',\n       '>= 200 DM / salary assignments for at least 1 year'], dtype=object),\n                                                                            array(['critical account/ other credits existing (not at this bank)',\n       'exi...\n                                                   'account_check_statustelephone',\n                                                   'account_check_statusforeign_worker',\n                                                   'credit_historypurpose',\n                                                   'credit_historysavings',\n                                                   'credit_historypresent_emp_since', ...]),\n                                                 ('ord', StandardScaler(),\n                                                  ['duration_in_month',\n                                                   'credit_amount',\n                                                   'installment_as_income_perc',\n                                                   'present_res_since', 'age',\n                                                   'credits_this_bank',\n                                                   'people_under_maintenance'])])),\n                ('logreg', LogisticRegression(max_iter=200))])\ny_test_pred = model_over.predict(X_secret_test)\ncm = confusion_matrix(y_secret_test, y_test_pred)\nprint(cm)\n[[138   4]\n [ 23  35]]\n\n\nX_secret_train.sex.value_counts(normalize=True)\nmale      0.58875\nfemale    0.41125\nName: sex, dtype: float64\n\n\nmeta_model.predict_proba(model_over['logreg'].coef_)\narray([[0.27493777, 0.72506223]])\n\nOn a donc un meta modèle qui ne dépends plus d’un model extraction et qui fonctionne bien. On va maintenant tracer pour différentes valeurs de epsilon et de data_norm les valeurs de prédiction du méta model et du recal.\n\ndef gen_model_diff(epsilon=1, data_norm=1):\n    model = Pipeline(\n        steps=[\n            ('relations', FunctionTransformer(add_relations)), \n            ('preprocession', preprocessor_comb),\n            # La seule différence avec model_over\n            ('logreg', diff.LogisticRegression(max_iter=200, epsilon=epsilon, data_norm=data_norm))\n        ]\n    )\n    return model\n\n\ndef mesure(model):\n    \n    y_test_pred = model.predict(X_test_secret)\n    cm = confusion_matrix(y_test_secret, y_test_pred)\n    \n    pred_homme = meta_model.predict_proba(model['logreg'].coef_)[0,1]\n    \n    recall = cm[0,0]/(cm[0,0]+cm[1,0])\n    specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n    precision = cm[0,0]/(cm[0,0]+cm[0,1])\n    accuracy =(cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n  \n    return recall, specificity, precision, accuracy, pred_homme\n\n\n# %%capture --no-display --no-stdout\nnew = False\n\ncolumns = ['recall', 'specificity', 'precision', 'accuracy', 'pred_homme', 'eps', 'data_norm']\n\nif new:\n    nb_test = 10\n    list_eps = np.logspace(-1, 2, 10)\n    list_data_norm = np.logspace(-1, 2, 10)\n    \n    X_secret = data_fix_p_homme(0.6, 1000)\n    y_secret = model_predict.predict(X_secret)\n    \n    X_train_secret, X_test_secret, y_train_secret, y_test_secret = model_selection.train_test_split(X_secret, \n                                                                                                    y_secret, \n                                                                                                    test_size=0.20)\n    def mesure(model):\n        model.fit(X_train_secret, y_train_secret)\n        y_test_pred = model.predict(X_test_secret)\n        cm = confusion_matrix(y_test_secret, y_test_pred)\n\n        recall = cm[0,0]/(cm[0,0]+cm[1,0])\n        specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n        precision = cm[0,0]/(cm[0,0]+cm[0,1])\n        accuracy = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n        \n        pred_homme = meta_model.predict_proba(model['logreg'].coef_)[0,1]\n\n        return recall, specificity, precision, accuracy, pred_homme\n\n    all_mesures = [(*mesure(gen_model_diff(eps, data_norm)), eps, data_norm) for eps in tqdm(list_eps) \n                 for data_norm in list_data_norm \n                     for _ in range(nb_test)]\n\n    df_mesures = pd.DataFrame(all_mesures, columns=columns)\n    \n    np.save('data/privacy/data_mesures.npy', df_mesures.to_numpy())\nelse:\n    df_mesures = pd.DataFrame(np.load('data/privacy/data_mesures.npy'), columns=columns)\n\n\n\n\n\n\n\n",
      "last_modified": "2021-07-31T23:08:25+02:00"
    }
  ],
  "collections": []
}
